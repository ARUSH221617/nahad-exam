"use server";

import { GoogleGenAI, FunctionCallingConfigMode, Type } from "@google/genai";
import prisma from "@/lib/prisma";
import { generateEmbedding } from "@/lib/ai";
import { createClient } from "@/lib/supabase/server";

export async function askQuestion(
  docId: string,
  question: string,
  history: { role: string; parts: string[] }[]
) {
  const supabase = await createClient();
  const { data: { user } } = await supabase.auth.getUser();
  const userId = user?.id ?? "anonymous";

  // 1. Retrieve context
  const doc = await prisma.document.findUnique({
    where: { id: docId },
  });

  if (!doc) throw new Error("Document not found");

  // Generate embedding for the question
  const questionEmbedding = await generateEmbedding(question);
  const embeddingString = `[${questionEmbedding.join(",")}]`;

  // Search for relevant chunks
  const chunks = await prisma.$queryRaw<Array<{ content: string; similarity: number }>>`
    SELECT content, 1 - (embedding <=> ${embeddingString}::vector) as similarity
    FROM "DocumentChunk"
    WHERE "documentId" = ${docId}
    ORDER BY embedding <=> ${embeddingString}::vector
    LIMIT 5;
  `;

  const context = chunks.map((c) => c.content).join("\n\n");
  const references = chunks.map((c, i) => `Source ${i+1}: ${c.content.substring(0, 50)}...`);

  // 2. Generate Answer with Function Calling
  const apiKey = process.env.GOOGLE_GENERATIVE_AI_API_KEY;
  if (!apiKey) {
    return {
      answer: "Error: API Key not found.",
      references: [],
    };
  }

  const ai = new GoogleGenAI({ apiKey });
  const model = "gemini-2.0-flash"; // Supports function calling well

  // Define the tool
  const saveExamTool = {
    name: "save_exam_result",
    description: "Saves the question and answer to the database history when a valid question is answered.",
    parameters: {
      type: Type.OBJECT,
      properties: {
        question: { type: Type.STRING, description: "The question asked by the user" },
        answer: { type: Type.STRING, description: "The answer generated by the model" },
      },
      required: ["question", "answer"],
    },
  };

  const config = {
    tools: [
      {
        functionDeclarations: [saveExamTool],
      },
    ],
    toolConfig: {
      functionCallingConfig: {
        mode: FunctionCallingConfigMode.AUTO,
      },
    },
  };

  const prompt = `You are an expert in Nahad exams. Using ONLY the provided context from the PDF, answer the user's question.
    Context:
    ${context}

    Question: ${question}

    Instructions:
    1. Answer the question in Persian (RTL). Format as Markdown.
    2. If the user's input is a question about the content, provide the answer.
    3. IMPORTANT: After answering, if you have successfully answered a valid question, YOU MUST CALL the "save_exam_result" function to save the record.
    4. If the user is just saying hello or the input is not a question, do not call the save function.
    5. If the answer is not in the context, say "پاسخ در متن یافت نشد" and do not save.

    If you call the tool, the system will execute it. Your final response should be the answer text.
    `;

  const contents = [
    ...history.map(h => ({ role: h.role === 'user' ? 'user' : 'model', parts: [{ text: h.parts.join('') }] })),
    {
      role: "user",
      parts: [
        {
          text: prompt,
        },
      ],
    },
  ];

  // We need to handle potential multi-turn if tool is called.
  // GenerateContent (non-stream) is easier for function calling handling in this one-shot wrapper.
  const result = await ai.models.generateContent({
    model,
    config,
    contents,
  });

  let answerText = "";

  // Handle function calls
  // @ts-ignore
  const functionCalls = result.functionCalls;

  if (functionCalls && functionCalls.length > 0) {
      for (const call of functionCalls) {
          if (call.name === "save_exam_result") {
              const args = call.args as { question: string, answer: string };
              console.log("Saving exam result via tool call:", args);

              // Execute the saving logic
              await prisma.exam.create({
                  data: {
                      question: args.question,
                      answer: args.answer,
                      references: JSON.stringify(references),
                      documentId: docId,
                      userId: userId,
                  }
              });

              answerText = args.answer;
          }
      }
  }

  // If the model didn't call the function (e.g. casual chat), or returned text alongside tool call.
  // Usually if tool is called, the model expects tool output.
  // But here we just want to ensure we get the text to display.
  // 'result.text' might be empty if it only called a function.
  // The 'save_exam_result' arg has the answer, so we used that.

  // However, often the model generates text AND calls the function, or just calls the function.
  // If result.text is present, use it. If not, and we have answer from tool arg, use that.

  if (result.text) {
      answerText = result.text;
  }

  // If we processed a save, we ensure answerText is set.
  // If we didn't save, answerText comes from result.text.

  // Note: Standard function calling flow requires sending tool output back to model.
  // But here we are using it as a "side effect" trigger.
  // If the model *only* returned a function call, we need to extract the answer from the arguments to show to the user.

  if (!answerText && functionCalls && functionCalls.length > 0) {
      // The model put the answer in the function arguments
      const args = functionCalls[0].args as { answer: string };
      answerText = args.answer;
  }

  return { answer: answerText, references };
}
